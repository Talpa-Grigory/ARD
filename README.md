## [CVPR 2025 Oral] Autoregressive Distillation of Diffusion Transformers <br><sub><sub> This repository is a re-implementation of the original work (ARD), recreated based on the author's memory. </sub></sub>
**[Yeongmin Kim](https://sites.google.com/view/yeongmin-space), Sotiris Anagnostidis, Yuming Du, Edgar Schoenfeld, Jonas Kohler, Markos Georgopoulos, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu**  

## Dependencies
The requirements for this code are the same as [DiT](https://github.com/facebookresearch/DiT).
